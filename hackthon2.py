# -*- coding: utf-8 -*-
"""Hackthon2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lB46Ef_RAKMXYBgK9_OxvdwWzIDFNSAL
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files
files.upload()

df = pd.read_csv("creditcard.csv")
print(df.info())
print(df['Class'].value_counts())

sns.countplot(x='Class', data=df)
plt.title("Fraud vs Normal Transaction Count")
plt.show()

from sklearn.model_selection import train_test_split
X = df.drop('Class', axis=1)
y = df['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

model = LogisticRegression()     # precisio, recall , f1-score and support
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

model = LogisticRegression()     # precision, recall , f1-score and support
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)
plt.show()

print(df.describe())

print(df.isnull().sum()) # Check for missing values

print("Shape:", df.shape) # Dataset shape (rows, columns)

# Histograms
df.hist(figsize=(10,8), bins=30)
plt.show()

pip install plotly # Plotly for interactive and dynamic visualizations.

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df = pd.read_csv("creditcard.csv") #Explore Relationship: Amount vs Class
                                   #Let’s see if fraudulent transactions tend to have different amounts.Distribution Plot
plt.figure(figsize=(10,5))
sns.boxplot(x='Class', y='Amount', data=df)
plt.title('Transaction Amount by Class')
plt.show()

plt.figure(figsize=(10,5))
sns.histplot(data=df, x='Time', hue='Class', bins=50, kde=True)  #Relationship: Time vs Class
                                                                 #The Time column represents seconds elapsed since the first transaction.
                                                                 # You can visualize when frauds tend to happen.
plt.title('Distribution of Transactions over Time by Class')
plt.show()

fraud = df[df['Class'] == 1]
non_fraud = df[df['Class'] == 0]    #To explore Relationship: V1–V28 vs Class

feature_means = pd.DataFrame({
    'Fraudulent': fraud.iloc[:, 1:29].mean(),
    'Non-Fraudulent': non_fraud.iloc[:, 1:29].mean()
})

feature_means.plot(kind='bar', figsize=(15,5), title='Mean of V1–V28 by Class')
plt.show()

corr = df.corr()['Class'].drop('Class')
plt.figure(figsize=(12,5))
corr.sort_values(ascending=False).plot(kind='bar')
plt.title('Correlation of Features with Class')
plt.show()

sns.scatterplot(data=df.sample(5000), x='V2', y='V21', hue='Class', alpha=0.7)
plt.title('Relationship between V2 and V4 by Class') # Visualize pairs of features to see if frauds form separate clusters.
plt.show()

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

features = df.drop('Class', axis=1)
X_scaled = StandardScaler().fit_transform(features)   # Use PCA or t-SNE to visualize all features in 2D.
y = df['Class']

pca = PCA(n_components=2)
pca_result = pca.fit_transform(X_scaled)

plt.figure(figsize=(8,6))
sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1], hue=y, alpha=0.5)
plt.title('PCA Projection of Transactions')
plt.show()

import pandas as pd
from collections import Counter

df = pd.read_csv('creditcard.csv')    # Before applying any technique, always quantify the imbalance.
                                       # Check the imbalance
print(Counter(df['Class']))
df['Class'].value_counts(normalize=True)

Counter({0: 284315, 1: 492})
# Exemple output
 #Fraud = 0.17%, Non-fraud = 99.83% — that’s a massive imbalance.

from sklearn.model_selection import train_test_split

X = df.drop('Class', axis=1)
y = df['Class']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42)        #Reduce the majority class to balance with the minority class.
X_res, y_res = rus.fit_resample(X_train, y_train)

print(Counter(y_res))

from imblearn.over_sampling import RandomOverSampler
                                    #Random Oversampling Duplicate samples of the minority class until classes are balanced.
ros = RandomOverSampler(random_state=42)
X_res, y_res = ros.fit_resample(X_train, y_train)

print(Counter(y_res))

import numpy as np
import matplotlib.pyplot as plt

importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(12,6))
plt.title("Feature Importances (Random Forest)")
plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.show()